Classification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models. 
Most classifiers in scikit-learn have a .predict_proba() method which returns the probability of a given sample being in a particular class. 
Having built a logistic regression model, you'll now evaluate its performance by plotting an ROC curve. 
In doing so, you'll make use of the .predict_proba() method and become familiar with its functionality.

# Import necessary modules
from sklearn.metrics import roc_curve 

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

When looking at your ROC curve, you may have noticed that the y-axis (True positive rate) is also known as recall. 
Indeed, in addition to the ROC curve, there are other ways to visually evaluate model performance. 
One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. 

As a reminder, precision and recall are defined as:
Precision=TP/(TP+FP)
Recall=TP/(TP+FN)

Say you have a binary classifier that in fact is just randomly making guesses. 
It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. 
The Area under this ROC curve would be 0.5. This is one way in which the AUCnis an informative metric to evaluate a model. 
If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!

# Import necessary modules
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Compute and print AUC score
print("AUC: {}".format(roc_auc_score(y_test, y_pred_prob)))

# Compute cross-validated AUC scores: cv_auc
cv_auc = cross_val_score(logreg, X, y, cv = 5, scoring = 'roc_auc')

# Print list of AUC scores
print("AUC scores computed using 5-fold cross-validation: {}".format(cv_auc))
